2021-03-04 09:53:26 [scrapy.utils.log] INFO: Scrapy 2.1.0 started (bot: ARGUS)
2021-03-04 09:53:26 [scrapy.utils.log] INFO: Versions: lxml 4.6.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 | packaged by conda-forge | (default, Dec 26 2019, 23:46:52) - [Clang 9.0.0 (tags/RELEASE_900/final)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Darwin-18.7.0-x86_64-i386-64bit
2021-03-04 09:53:26 [scrapy.crawler] INFO: Overridden settings:
{'AJAXCRAWL_ENABLED': True,
 'BOT_NAME': 'ARGUS',
 'COMPRESSION_ENABLED': False,
 'CONCURRENT_ITEMS': 200,
 'CONCURRENT_REQUESTS': 100,
 'COOKIES_ENABLED': False,
 'DOWNLOAD_MAXSIZE': 10000000,
 'DOWNLOAD_TIMEOUT': 20,
 'LOG_FILE': 'logs/ARGUS/dualspider/1a495e047cb211ebaf3134363b7bab66.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'ARGUS.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 30,
 'RETRY_ENABLED': False,
 'SPIDER_MODULES': ['ARGUS.spiders']}
2021-03-04 09:53:26 [scrapy.extensions.telnet] INFO: Telnet Password: ac087899d923629a
2021-03-04 09:53:26 [py.warnings] WARNING: /Users/parnianshahkar/Downloads/miniconda3/lib/python3.7/site-packages/scrapy/extensions/feedexport.py:210: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details
  exporter = cls(crawler)

2021-03-04 09:53:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2021-03-04 09:53:30 [twisted] CRITICAL: Unhandled error in Deferred:
2021-03-04 09:53:31 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/Users/parnianshahkar/Downloads/miniconda3/lib/python3.7/site-packages/twisted/internet/defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "/Users/parnianshahkar/Downloads/miniconda3/lib/python3.7/site-packages/scrapy/crawler.py", line 86, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/Users/parnianshahkar/Downloads/miniconda3/lib/python3.7/site-packages/scrapy/crawler.py", line 98, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/Users/parnianshahkar/Downloads/miniconda3/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 49, in from_crawler
    spider = cls(*args, **kwargs)
  File "/Users/parnianshahkar/Documents/KOF/Task2/ARGUS-main/ARGUS/spiders/dualspider.py", line 39, in __init__
    data = pd.read_csv(url_chunk, delimiter="\t", encoding="utf-8", error_bad_lines=False, engine="python")
  File "/Users/parnianshahkar/Downloads/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/Users/parnianshahkar/Downloads/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "/Users/parnianshahkar/Downloads/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "/Users/parnianshahkar/Downloads/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py", line 1189, in _make_engine
    self._engine = klass(self.f, **self.options)
  File "/Users/parnianshahkar/Downloads/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py", line 2387, in __init__
    memory_map=self.memory_map,
  File "/Users/parnianshahkar/Downloads/miniconda3/lib/python3.7/site-packages/pandas/io/common.py", line 493, in get_handle
    f = open(path_or_buf, mode, encoding=encoding, errors=errors, newline="")
FileNotFoundError: [Errno 2] No such file or directory: '/Users/parnianshahkar/Documents/KOF/Task2/ARGUS-main/chunks/url_chunk_p1.csv'
